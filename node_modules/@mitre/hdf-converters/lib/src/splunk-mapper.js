"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.SplunkMapper = exports.replaceKeyValueDescriptions = exports.consolidatePayloads = exports.mapHash = exports.groupBy = void 0;
const axios_1 = __importDefault(require("axios"));
const lodash_1 = __importDefault(require("lodash"));
const global_1 = require("./utils/global");
const splunk_tools_1 = require("./utils/splunk-tools");
const MAPPER_NAME = 'Splunk2HDF';
let logger = (0, global_1.createWinstonLogger)('Splunk2HDF');
function groupBy(items, keyGetter) {
    const result = {};
    for (const i of items) {
        const key = keyGetter(i);
        const corrList = result[key];
        if (corrList) {
            corrList.push(i);
        }
        else {
            result[key] = [i];
        }
    }
    return result;
}
exports.groupBy = groupBy;
function mapHash(old, mapFunction) {
    const result = {};
    for (const key in old) {
        result[key] = mapFunction(old[key]);
    }
    return result;
}
exports.mapHash = mapHash;
function consolidatePayloads(payloads) {
    const grouped = groupBy(payloads, (pl) => pl.meta.guid);
    const built = mapHash(grouped, consolidateFilePayloads);
    return Object.values(built);
}
exports.consolidatePayloads = consolidatePayloads;
function replaceKeyValueDescriptions(controls) {
    return controls.map((control) => {
        if (control.descriptions && !Array.isArray(control.descriptions)) {
            const extractedDescriptions = [];
            for (const [key, value] of Object.entries(control.descriptions)) {
                extractedDescriptions.push({ label: key, data: value });
            }
            control.descriptions = extractedDescriptions;
        }
        return control;
    });
}
exports.replaceKeyValueDescriptions = replaceKeyValueDescriptions;
function consolidateFilePayloads(filePayloads) {
    var _a;
    const subtypes = groupBy(filePayloads, (event) => event.meta.subtype);
    const execEvents = (subtypes['header'] ||
        []);
    const profileEvents = (subtypes['profile'] ||
        []);
    const controlEvents = (subtypes['control'] ||
        []);
    logger.debug(`Have ${execEvents.length} execution events`);
    logger.debug(`Have ${profileEvents.length} profile events`);
    logger.debug(`Have ${controlEvents.length} control events`);
    if (execEvents.length !== 1) {
        throw new Error(`Incorrect # of Evaluation events. Expected 1, got ${execEvents.length}`);
    }
    const exec = execEvents[0];
    (_a = exec.profiles) === null || _a === void 0 ? void 0 : _a.push(...profileEvents);
    const shaGroupedControls = groupBy(controlEvents, (ctrl) => ctrl.meta.profile_sha256);
    for (const profile of profileEvents) {
        profile.controls = [];
        const sha = profile.meta.profile_sha256;
        logger.debug(`Adding controls for profile with SHA256: ${sha}`);
        const corrControls = shaGroupedControls[sha] || [];
        profile.controls.push(...replaceKeyValueDescriptions(corrControls));
        logger.debug(`Added ${profile.controls.length} controls to profile with SHA256 ${sha}`);
    }
    return exec;
}
function unixTimeToDate(unixTime) {
    return new Date(parseFloat(unixTime) * 1000);
}
class SplunkMapper {
    constructor(config, logService, loggingLevel) {
        this.config = config;
        this.axiosInstance = axios_1.default.create({ params: { output_mode: 'json' } });
        this.hostname = (0, splunk_tools_1.generateHostname)(config);
        if (logService) {
            logger = logService;
        }
        else {
            logger = (0, global_1.createWinstonLogger)(MAPPER_NAME, loggingLevel || 'debug');
        }
        logger.debug(`Initialized ${this.constructor.name} successfully`);
    }
    async createJob(query) {
        logger.debug(`Creating job for query: ${query}`);
        let jobSID;
        try {
            jobSID = await this.axiosInstance.post(`${this.hostname}/services/search/jobs`, `exec_mode=blocking&search=${query}`);
        }
        catch (error) {
            const errorCode = (0, splunk_tools_1.handleSplunkErrorResponse)(error);
            throw new Error(`Failed to create search job - ${errorCode}`);
        }
        if (lodash_1.default.has(jobSID, ['data', 'sid'])) {
            return jobSID.data.sid;
        }
        else {
            throw new Error('Failed to create search job - Malformed search job creation response received');
        }
    }
    async trackJob(job) {
        const badState = new Set([
            'PAUSE',
            'INTERNAL_CANCEL',
            'USER_CANCEL',
            'BAD_INPUT_CANCEL',
            'QUIT',
            'FAILED'
        ]);
        const searchJobTimeout = 120000;
        const searchJobPing = 50;
        let queryStatus;
        let continuePing = true;
        const queryTimer = setTimeout(() => {
            continuePing = false;
            clearTimeout(queryTimer);
            throw new Error('Search job timed out - Unable to retrieve query');
        }, searchJobTimeout);
        const awaitJob = setInterval(async () => {
            try {
                queryStatus = await this.axiosInstance.get(`${this.hostname}/services/search/jobs/${job}`);
            }
            catch (error) {
                clearTimeout(queryTimer);
                clearInterval(awaitJob);
                throw new Error(`Failed search job - ${(0, splunk_tools_1.handleSplunkErrorResponse)(error)}`);
            }
            if (lodash_1.default.has(queryStatus, 'data.entry[0].content')) {
                if (queryStatus.data.entry.length !== 1) {
                    clearTimeout(queryTimer);
                    clearInterval(awaitJob);
                    throw new Error(`Failed search job - Detected malformed entry field length ${queryStatus.data.entry.length}`);
                }
                if (queryStatus.data.entry[0].content.dispatchState === 'DONE' &&
                    queryStatus.data.entry[0].content.isDone) {
                    clearTimeout(queryTimer);
                    clearInterval(awaitJob);
                }
                else if (badState.has(queryStatus.data.entry[0].content.dispatchState)) {
                    clearTimeout(queryTimer);
                    clearInterval(awaitJob);
                    throw new Error(`Failed search job - Detected dispatch state ${queryStatus.data.entry[0].content.dispatchState}`);
                }
            }
            else {
                clearTimeout(queryTimer);
                clearInterval(awaitJob);
                throw new Error('Failed search job - Malformed search job response received');
            }
            if (!continuePing) {
                clearInterval(awaitJob);
            }
        }, searchJobPing);
    }
    parseSplunkResponse(query, results) {
        logger.info(`Got results for query: ${query}`);
        const objects = [];
        let rawDataIndex = results === null || results === void 0 ? void 0 : results.fields.findIndex((field) => field.toLowerCase() === '_raw');
        if (rawDataIndex === -1) {
            logger.error(`Field _raw not found, using default index 3`);
            rawDataIndex = 3;
        }
        logger.debug(`Got field _raw at index ${rawDataIndex}`);
        let indexTimeIndex = results === null || results === void 0 ? void 0 : results.fields.findIndex((field) => field.toLowerCase() === '_indextime');
        if (indexTimeIndex === -1) {
            logger.error(`Field _indextime not found, using default index 2`);
            indexTimeIndex = 2;
        }
        logger.debug(`Got field _indextime at index ${indexTimeIndex}`);
        logger.verbose(`Parsing data returned by Splunk and appending timestamps`);
        for (const value of results.rows) {
            let object;
            try {
                object = JSON.parse(value[rawDataIndex]);
            }
            catch {
                throw new Error('Unable to parse file. Have you configured EVENT_BREAKER? See https://github.com/mitre/saf/wiki/Splunk-Configuration');
            }
            try {
                lodash_1.default.set(object, 'meta.parse_time', unixTimeToDate(value[indexTimeIndex]).toISOString());
            }
            catch {
                lodash_1.default.set(object, 'meta.parse_time', new Date().toISOString());
            }
            objects.push(object);
        }
        logger.debug('Successfully parsed and added timestamps');
        return objects;
    }
    async queryData(query) {
        let queryJob;
        const authToken = await (0, splunk_tools_1.checkSplunkCredentials)(this.config);
        this.axiosInstance.defaults.headers.common['Authorization'] = `Bearer ${authToken}`;
        const job = await this.createJob(query);
        await this.trackJob(job);
        try {
            const returnCount = 0;
            queryJob = await this.axiosInstance.get(`${this.hostname}/services/search/v2/jobs/${job}/results`, {
                params: { count: returnCount, output_mode: 'json_rows' }
            });
        }
        catch (error) {
            throw new Error(`Failed search job - ${(0, splunk_tools_1.handleSplunkErrorResponse)(error)}`);
        }
        if (lodash_1.default.has(queryJob, ['data'])) {
            return this.parseSplunkResponse(query, queryJob.data);
        }
        else {
            throw new Error('Failed search job - Malformed search job results response received');
        }
    }
    async toHdf(guid) {
        logger.info(`Starting conversion of GUID ${guid}`);
        await (0, splunk_tools_1.checkSplunkCredentials)(this.config);
        logger.info(`Credentials valid, querying data for ${guid}`);
        const executionData = await this.queryData(`search index="*" meta.guid="${guid}"`);
        logger.info(`Data received, consolidating payloads for ${executionData.length} items`);
        return consolidatePayloads(executionData)[0];
    }
}
exports.SplunkMapper = SplunkMapper;
//# sourceMappingURL=splunk-mapper.js.map