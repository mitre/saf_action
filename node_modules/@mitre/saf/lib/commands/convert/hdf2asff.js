"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
const tslib_1 = require("tslib");
const core_1 = require("@oclif/core");
const fs_1 = tslib_1.__importDefault(require("fs"));
const https_1 = tslib_1.__importDefault(require("https"));
const hdf_converters_1 = require("@mitre/hdf-converters");
const path_1 = tslib_1.__importDefault(require("path"));
const aws_sdk_1 = tslib_1.__importDefault(require("aws-sdk"));
const global_1 = require("../../utils/global");
const lodash_1 = tslib_1.__importDefault(require("lodash"));
class HDF2ASFF extends core_1.Command {
    async run() {
        var _a;
        const { flags } = await this.parse(HDF2ASFF);
        const converted = new hdf_converters_1.FromHdfToAsffMapper(JSON.parse(fs_1.default.readFileSync(flags.input, 'utf8')), {
            awsAccountId: flags.accountId,
            region: flags.region,
            regionAttribute: flags.specifyRegionAttribute,
            target: flags.target,
            input: flags.input,
        }).toAsff();
        if (flags.output) {
            const convertedSlices = (0, global_1.sliceIntoChunks)(converted, 100);
            const outputFolder = ((_a = flags.output) === null || _a === void 0 ? void 0 : _a.replace('.json', '')) || 'asff-output';
            fs_1.default.mkdirSync(outputFolder);
            if (convertedSlices.length === 1) {
                const outfilePath = path_1.default.join(outputFolder, (0, global_1.checkSuffix)(flags.output));
                fs_1.default.writeFileSync(outfilePath, JSON.stringify(convertedSlices[0]));
            }
            else {
                convertedSlices.forEach((slice, index) => {
                    const outfilePath = path_1.default.join(outputFolder, `${(0, global_1.checkSuffix)(flags.output || '').replace('.json', '')}.p${index}.json`);
                    fs_1.default.writeFileSync(outfilePath, JSON.stringify(slice));
                });
            }
        }
        if (flags.upload) {
            const profileInfoFinding = converted.pop();
            const convertedSlices = (0, global_1.sliceIntoChunks)(converted, 100);
            if (flags.insecure) {
                console.warn('WARNING: Using --insecure will make all connections to AWS open to MITM attacks, if possible pass a certificate file with --certificate');
            }
            const clientOptions = {
                region: flags.region,
            };
            aws_sdk_1.default.config.update({
                httpOptions: {
                    agent: new https_1.default.Agent({
                        rejectUnauthorized: !flags.insecure,
                        ca: flags.certificate ? fs_1.default.readFileSync(flags.certificate, 'utf8') : undefined,
                    }),
                },
            });
            const client = new aws_sdk_1.default.SecurityHub(clientOptions);
            Promise.all(convertedSlices.map(async (chunk) => {
                var _a;
                try {
                    const result = await client.batchImportFindings({ Findings: chunk }).promise();
                    console.log(`Uploaded ${chunk.length} controls. Success: ${result.SuccessCount}, Fail: ${result.FailedCount}`);
                    if ((_a = result.FailedFindings) === null || _a === void 0 ? void 0 : _a.length) {
                        console.error(`Failed to upload ${result.FailedCount} Findings`);
                        console.log(result.FailedFindings);
                    }
                }
                catch (error) {
                    if (typeof error === 'object' && lodash_1.default.get(error, 'code', false) === 'NetworkingError') {
                        console.error(`Failed to upload controls: ${error}; Using --certificate to provide your own SSL intermediary certificate (in .crt format) or use the flag --insecure to ignore SSL might resolve this issue`);
                    }
                    else {
                        console.error(`Failed to upload controls: ${error}`);
                    }
                }
            })).then(async () => {
                var _a;
                if (profileInfoFinding) {
                    profileInfoFinding.UpdatedAt = new Date().toISOString();
                    const result = await client.batchImportFindings({ Findings: [profileInfoFinding] }).promise();
                    console.info(`Statistics: ${profileInfoFinding.Description}`);
                    console.info(`Uploaded Results Set Info Finding(s) - Success: ${result.SuccessCount}, Fail: ${result.FailedCount}`);
                    if ((_a = result.FailedFindings) === null || _a === void 0 ? void 0 : _a.length) {
                        console.error(`Failed to upload ${result.FailedCount} Results Set Info Finding`);
                        console.log(result.FailedFindings);
                    }
                }
            });
        }
    }
}
exports.default = HDF2ASFF;
HDF2ASFF.usage = 'convert hdf2asff -i, --input=HDF-JSON -o, --output=ASFF-JSON-Folder -a, --accountId=accountId -r, --region=region -t, --target=target -u, --upload';
HDF2ASFF.description = 'Translate a Heimdall Data Format JSON file into AWS Security Findings Format JSON file(s) and/or upload to AWS Security Hub';
HDF2ASFF.examples = ['saf convert hdf2asff -i rhel7-scan_02032022A.json -a 123456789 -r us-east-1 -t rhel7_example_host -o rhel7.asff', 'saf convert hdf2asff -i rds_mysql_i123456789scan_03042022A.json -a 987654321 -r us-west-1 -t Instance_i123456789 -u', 'saf convert hdf2asff -i snyk_acme_project5_hdf_04052022A.json -a 2143658798 -r us-east-1 -t acme_project5 -o snyk_acme_project5 -u'];
HDF2ASFF.flags = {
    help: core_1.Flags.help({ char: 'h' }),
    accountId: core_1.Flags.string({ char: 'a', required: true, description: 'AWS Account ID' }),
    region: core_1.Flags.string({ char: 'r', required: true, description: 'SecurityHub Region' }),
    specifyRegionAttribute: core_1.Flags.boolean({ char: 'R', required: false, description: 'Manually specify the top-level `Region` attribute - SecurityHub populates this attribute automatically and prohibits one from updating it using `BatchImportFindings` or `BatchUpdateFindings`' }),
    input: core_1.Flags.string({ char: 'i', required: true, description: 'Input HDF JSON File' }),
    target: core_1.Flags.string({ char: 't', required: true, description: 'Unique name for target to track findings across time' }),
    upload: core_1.Flags.boolean({ char: 'u', required: false, description: 'Upload findings to AWS Security Hub' }),
    output: core_1.Flags.string({ char: 'o', required: false, description: 'Output ASFF JSON Folder' }),
    insecure: core_1.Flags.boolean({ char: 'I', required: false, default: false, description: 'Disable SSL verification, this is insecure.' }),
    certificate: core_1.Flags.string({ char: 'C', required: false, description: 'Trusted signing certificate file' }),
};
