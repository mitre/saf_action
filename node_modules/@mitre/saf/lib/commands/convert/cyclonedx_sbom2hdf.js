"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
const tslib_1 = require("tslib");
const core_1 = require("@oclif/core");
const fs_1 = tslib_1.__importDefault(require("fs"));
const hdf_converters_1 = require("@mitre/hdf-converters");
const global_1 = require("../../utils/global");
const baseCommand_1 = require("../../utils/oclif/baseCommand");
class CycloneDXSBOM2HDF extends baseCommand_1.BaseCommand {
    static usage = '<%= command.id %> -i <cyclonedx_sbom-json> -o <hdf-scan-results-json> [-h] [-w]';
    static description = 'Translate a CycloneDX SBOM report into an HDF results set';
    static examples = ['<%= config.bin %> <%= command.id %> -i cyclonedx_sbom.json -o output-hdf-name.json'];
    static flags = {
        input: core_1.Flags.string({
            char: 'i',
            required: true,
            description: 'Input CycloneDX SBOM file',
        }),
        output: core_1.Flags.string({
            char: 'o',
            required: true,
            description: 'Output HDF JSON file',
        }),
        includeRaw: core_1.Flags.boolean({
            char: 'w',
            required: false,
            description: 'Include raw input file in HDF JSON file',
        }),
    };
    async run() {
        const { flags } = await this.parse(CycloneDXSBOM2HDF);
        // Check for correct input type
        const data = fs_1.default.readFileSync(flags.input, 'utf8');
        (0, global_1.checkInput)({ data, filename: flags.input }, 'cyclonedx_sbom', 'CycloneDX SBOM output file');
        const converter = new hdf_converters_1.CycloneDXSBOMResults(data, flags.includeRaw);
        fs_1.default.writeFileSync((0, global_1.checkSuffix)(flags.output), JSON.stringify(converter.toHdf(), null, 2));
    }
}
exports.default = CycloneDXSBOM2HDF;
//# sourceMappingURL=cyclonedx_sbom2hdf.js.map