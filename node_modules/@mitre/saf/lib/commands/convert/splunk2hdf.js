"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
const tslib_1 = require("tslib");
const core_1 = require("@oclif/core");
const splunk_mapper_1 = require("@mitre/hdf-converters/lib/src/splunk-mapper");
const table_1 = require("table");
const lodash_1 = tslib_1.__importDefault(require("lodash"));
const fs_1 = tslib_1.__importDefault(require("fs"));
const path_1 = tslib_1.__importDefault(require("path"));
const global_1 = require("../../utils/global");
const logging_1 = require("../../utils/logging");
const baseCommand_1 = require("../../utils/oclif/baseCommand");
class Splunk2HDF extends baseCommand_1.BaseCommand {
    static usage = '<%= command.id %> -H <host> -I <index> [-h] [-P <port>] [-s http|https]\n'
        + '(-u <username> -p <password> | -t <token>) [-L info|warn|debug|verbose] [-i <filename/GUID>... -o <hdf-output-folder>]';
    static description = 'Pull HDF data from your Splunk instance back into an HDF file';
    static examples = [
        '<%= config.bin %> <%= command.id %> -H 127.0.0.1 -u admin -p Valid_password!\n'
            + '-I hdf -i some-file-in-your-splunk-instance.json -i yBNxQsE1mi4f3mkjtpap5YxNTttpeG -o output-folder',
    ];
    static flags = {
        host: core_1.Flags.string({
            char: 'H',
            required: true,
            description: 'Splunk Hostname or IP',
        }),
        port: core_1.Flags.integer({
            char: 'P',
            required: false,
            description: 'Splunk management port (also known as the Universal Forwarder port)',
            default: 8089,
        }),
        scheme: core_1.Flags.string({
            char: 's',
            required: false,
            description: 'HTTP Scheme used for communication with splunk',
            default: 'https',
            options: ['http', 'https'],
        }),
        username: core_1.Flags.string({
            char: 'u',
            required: false,
            description: 'Your Splunk username',
            exclusive: ['token'],
        }),
        password: core_1.Flags.string({
            char: 'p',
            required: false,
            description: 'Your Splunk password',
            exclusive: ['token'],
        }),
        token: core_1.Flags.string({
            char: 't',
            required: false,
            description: 'Your Splunk API Token',
            exclusive: ['username', 'password'],
        }),
        index: core_1.Flags.string({
            char: 'I',
            required: true,
            description: 'Splunk index to query HDF data from',
        }),
        input: core_1.Flags.string({
            char: 'i',
            multiple: true,
            required: false,
            description: 'GUID(s) or Filename(s) of files from Splunk to convert',
        }),
        output: core_1.Flags.string({
            char: 'o',
            required: false,
            description: 'Output HDF JSON Folder',
        }),
    };
    async searchExecutions(mapper, filename, index) {
        return mapper.queryData(`search index="${index || '*'}" meta.filename="${filename || '*'}" meta.subtype="header" | head 100`);
    }
    async run() {
        const { flags } = await this.parse(Splunk2HDF);
        const logger = (0, logging_1.createWinstonLogger)('splunk2hdf', flags.logLevel);
        if (!(flags.username && flags.password) && !flags.token) {
            logger.error('Please provide either a Username and Password or a Splunk token');
            throw new Error('Please provide either a Username and Password or a Splunk token');
        }
        const mapper = new splunk_mapper_1.SplunkMapper({
            host: flags.host,
            port: flags.port,
            scheme: flags.scheme, // Types as defined by flags
            username: flags.username,
            password: flags.password,
            sessionKey: flags.token,
            index: flags.index,
        }, logger);
        if (flags.input && flags.output) {
            const outputFolder = flags.output?.replace('.json', '') || 'asff-output';
            fs_1.default.mkdirSync(outputFolder);
            flags.input.forEach(async (input) => {
                // If we have a GUID
                if (/^(\w){30}$/.test(input)) {
                    const hdf = await mapper.toHdf(input);
                    // Rename example.json -> example-p9dwG2kdSoHsYdyF2dMytUmljgOHD5.json and put into the outputFolder
                    fs_1.default.writeFileSync(path_1.default.join(outputFolder, (0, global_1.basename)(lodash_1.default.get(hdf, 'meta.filename', '').replace(/\.json$/, '')
                        + lodash_1.default.get(hdf, 'meta.guid')
                        + '.json')), JSON.stringify(hdf, null, 2));
                }
                else {
                    // If we have a filename
                    const executions = await this.searchExecutions(mapper, input);
                    executions.forEach(async (execution) => {
                        const hdf = await mapper.toHdf(lodash_1.default.get(execution, 'meta.guid'));
                        fs_1.default.writeFileSync(path_1.default.join(outputFolder, (0, global_1.basename)(lodash_1.default.get(hdf, 'meta.filename', '').replace(/\.json$/, '')
                            + lodash_1.default.get(hdf, 'meta.guid')
                            + '.json')), JSON.stringify(hdf, null, 2));
                    });
                }
            });
        }
        else if (flags.input && !flags.output) {
            logger.error('Please provide an output HDF folder');
            throw new Error('Please provide an output HDF folder');
        }
        else {
            const availableExecutionsTable = [['File Name', 'GUID', 'Imported At']];
            const executionsAvailable = await this.searchExecutions(mapper, '*');
            executionsAvailable.forEach((execution) => {
                availableExecutionsTable.push([
                    lodash_1.default.get(execution, 'meta.filename') || '',
                    lodash_1.default.get(execution, 'meta.guid') || '',
                    lodash_1.default.get(execution, 'meta.parse_time') || '',
                ]);
            });
            if (availableExecutionsTable.length === 1) {
                logger.warn('No executions found in the provided Splunk instance');
            }
            else {
                console.log('No filename or GUID provided (-i), available executions are:');
                console.log((0, table_1.table)(availableExecutionsTable));
            }
        }
    }
}
exports.default = Splunk2HDF;
//# sourceMappingURL=splunk2hdf.js.map