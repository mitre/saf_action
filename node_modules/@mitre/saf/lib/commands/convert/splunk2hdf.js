"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
const tslib_1 = require("tslib");
const core_1 = require("@oclif/core");
const splunk_mapper_1 = require("@mitre/hdf-converters/lib/src/splunk-mapper");
const table_1 = require("table");
const logging_1 = require("../../utils/logging");
const lodash_1 = tslib_1.__importDefault(require("lodash"));
const fs_1 = tslib_1.__importDefault(require("fs"));
const path_1 = tslib_1.__importDefault(require("path"));
class Splunk2HDF extends core_1.Command {
    async searchExecutions(mapper, filename, index) {
        return mapper.queryData(`search index="${index ? index : '*'}" meta.filename="${filename ? filename : '*'}" meta.subtype="header" | head 100`);
    }
    async run() {
        var _a;
        const { flags } = await this.parse(Splunk2HDF);
        const logger = (0, logging_1.createWinstonLogger)('splunk2hdf', flags.logLevel);
        if (!(flags.username && flags.password) && !flags.token) {
            logger.error('Please provide either a Username and Password or a Splunk token');
            throw new Error('Please provide either a Username and Password or a Splunk token');
        }
        const mapper = new splunk_mapper_1.SplunkMapper({
            host: flags.host,
            port: flags.port,
            scheme: flags.scheme,
            username: flags.username,
            password: flags.password,
            sessionKey: flags.token,
            index: flags.index,
        }, false, logger);
        if (flags.input && flags.output) {
            const outputFolder = ((_a = flags.output) === null || _a === void 0 ? void 0 : _a.replace('.json', '')) || 'asff-output';
            fs_1.default.mkdirSync(outputFolder);
            flags.input.forEach(async (input) => {
                // If we have a GUID
                if (/^(\w){30}$/.test(input)) {
                    const hdf = await mapper.toHdf(input);
                    // Rename example.json -> example-p9dwG2kdSoHsYdyF2dMytUmljgOHD5.json and put into the outputFolder
                    fs_1.default.writeFileSync(path_1.default.join(outputFolder, lodash_1.default.get(hdf, 'meta.filename').replace(/\.json$/, '') + lodash_1.default.get(hdf, 'meta.guid') + '.json'), JSON.stringify(hdf, null, 2));
                }
                else {
                    // If we have a filename
                    const executions = await this.searchExecutions(mapper, input);
                    executions.forEach(async (execution) => {
                        const hdf = await mapper.toHdf(lodash_1.default.get(execution, 'meta.guid'));
                        fs_1.default.writeFileSync(path_1.default.join(outputFolder, lodash_1.default.get(hdf, 'meta.filename').replace(/\.json$/, '') + lodash_1.default.get(hdf, 'meta.guid') + '.json'), JSON.stringify(hdf, null, 2));
                    });
                }
            });
        }
        else if (flags.input && !flags.output) {
            logger.error('Please provide an output HDF folder');
            throw new Error('Please provide an output HDF folder');
        }
        else {
            const availableExecutionsTable = [
                ['File Name', 'GUID', 'Imported At'],
            ];
            const executionsAvailable = await this.searchExecutions(mapper, '*');
            executionsAvailable.forEach(execution => {
                availableExecutionsTable.push([lodash_1.default.get(execution, 'meta.filename') || null, lodash_1.default.get(execution, 'meta.guid') || null, lodash_1.default.get(execution, 'meta.parse_time') || null]);
            });
            if (availableExecutionsTable.length === 1) {
                logger.warn('No executions found in the provided Splunk instance');
            }
            else {
                console.log('No filename or GUID provided (-i), available executions are:');
                console.log((0, table_1.table)(availableExecutionsTable));
            }
        }
    }
}
exports.default = Splunk2HDF;
Splunk2HDF.usage = 'splunk2hdf -i, --input=<filename/GUID> -H, --host -P, --port -p, --protocol -t, --token -i, --index -o, --output=<output-folder>';
Splunk2HDF.description = 'Pull HDF data from your Splunk instance back into an HDF file';
Splunk2HDF.flags = {
    help: core_1.Flags.help({ char: 'h' }),
    host: core_1.Flags.string({ char: 'H', required: true, description: 'Splunk Hostname or IP' }),
    port: core_1.Flags.integer({ char: 'P', required: false, description: 'Splunk management port (also known as the Universal Forwarder port)', default: 8089 }),
    scheme: core_1.Flags.string({ char: 's', required: false, description: 'HTTP Scheme used for communication with splunk', default: 'https', options: ['http', 'https'] }),
    username: core_1.Flags.string({ char: 'u', required: false, description: 'Your Splunk username', exclusive: ['token'] }),
    password: core_1.Flags.string({ char: 'p', required: false, description: 'Your Splunk password', exclusive: ['token'] }),
    token: core_1.Flags.string({ char: 't', required: false, description: 'Your Splunk API Token', exclusive: ['username', 'password'] }),
    index: core_1.Flags.string({ char: 'I', required: true, description: 'Splunk index to query HDF data from' }),
    logLevel: core_1.Flags.string({ char: 'L', required: false, default: 'info', options: ['info', 'warn', 'debug', 'verbose'] }),
    input: core_1.Flags.string({ char: 'i', multiple: true, required: false, description: 'GUID(s) or Filename(s) of files to convert' }),
    output: core_1.Flags.string({ char: 'o', required: false, description: 'Output HDF JSON Folder' }),
};
Splunk2HDF.examples = ['saf convert splunk2hdf -H 127.0.0.1 -u admin -p Valid_password! -I hdf -i some-file-in-your-splunk-instance.json yBNxQsE1mi4f3mkjtpap5YxNTttpeG -o output-folder'];
