"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
const tslib_1 = require("tslib");
const core_1 = require("@oclif/core");
const hdf_converters_1 = require("@mitre/hdf-converters");
const global_1 = require("../../utils/global");
const fs_1 = tslib_1.__importDefault(require("fs"));
const logging_1 = require("../../utils/logging");
class HDF2Splunk extends core_1.Command {
    async run() {
        const { flags } = await this.parse(HDF2Splunk);
        const logger = (0, logging_1.createWinstonLogger)('hdf2splunk', flags.logLevel);
        if (!(flags.username && flags.password) && !flags.token) {
            logger.error('Please provide either a Username and Password or a Splunk token');
            throw new Error('Please provide either a Username and Password or a Splunk token');
        }
        logger.warn('Please ensure the necessary configuration changes for your Splunk server have been configured to prevent data loss. See https://github.com/mitre/saf/wiki/Splunk-Configuration');
        const inputFile = JSON.parse(fs_1.default.readFileSync(flags.input, 'utf8'));
        logger.info(`Input File "${(0, global_1.convertFullPathToFilename)(flags.input)}": ${(0, logging_1.getHDFSummary)(inputFile)}`);
        await new hdf_converters_1.FromHDFToSplunkMapper(inputFile, logger).toSplunk({
            host: flags.host,
            port: flags.port,
            scheme: flags.scheme,
            username: flags.username,
            password: flags.password,
            sessionKey: flags.token,
            index: flags.index,
        }, (0, global_1.convertFullPathToFilename)(flags.input));
    }
}
HDF2Splunk.usage = 'convert hdf2splunk -i <hdf-scan-results-json> -H <host> -I <index> [-h] [-P <port>] [-s http|https] [-u <username> | -t <token>] [-p <password>] [-L info|warn|debug|verbose]';
HDF2Splunk.description = 'Translate and upload a Heimdall Data Format JSON file into a Splunk server';
HDF2Splunk.flags = {
    help: core_1.Flags.help({ char: 'h' }),
    input: core_1.Flags.string({ char: 'i', required: true, description: 'Input HDF file' }),
    host: core_1.Flags.string({ char: 'H', required: true, description: 'Splunk Hostname or IP' }),
    port: core_1.Flags.integer({ char: 'P', required: false, description: 'Splunk management port (also known as the Universal Forwarder port)', default: 8089 }),
    scheme: core_1.Flags.string({ char: 's', required: false, description: 'HTTP Scheme used for communication with splunk', default: 'https', options: ['http', 'https'] }),
    username: core_1.Flags.string({ char: 'u', required: false, description: 'Your Splunk username', exclusive: ['token'] }),
    password: core_1.Flags.string({ char: 'p', required: false, description: 'Your Splunk password', exclusive: ['token'] }),
    token: core_1.Flags.string({ char: 't', required: false, description: 'Your Splunk API Token', exclusive: ['username', 'password'] }),
    index: core_1.Flags.string({ char: 'I', required: true, description: 'Splunk index to import HDF data into' }),
    logLevel: core_1.Flags.string({ char: 'L', required: false, default: 'info', options: ['info', 'warn', 'debug', 'verbose'] }),
};
HDF2Splunk.examples = ['saf convert hdf2splunk -i rhel7-results.json -H 127.0.0.1 -u admin -p Valid_password! -I hdf', 'saf convert hdf2splunk -i rhel7-results.json -H 127.0.0.1 -t your.splunk.token -I hdf'];
exports.default = HDF2Splunk;
